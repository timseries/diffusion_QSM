    //Begin old fullpass process, Ax_b = A * x - b, Dx = D * x
#ifdef USE_OPENCL
    P->profile1.kern_time = 0;
    cl_size(P->cl, P->dN, 0, P->threads, rank);  //Resize
    if (first)
    {
      // Write initial x buffer
      cl_enqueue_write(P->cl, P->cl_x, P->cl_size_fg, P->x);
      first = false;
    }
    cl_set_arg(cl, P->kernel_iterate1, 13, P->dspec.start);
    cl_set_arg(cl, P->kernel_iterate1, 14, P->dspec.end);
    // Perform the operations
    cl_set_arg(cl, P->kernel_zero, 4, P->dN);
    cl_enqueue_kernel(P->cl, P->kernel_zero, NULL);
    //Only have x values on 2nd and subsequent iterations
    if (iteration == 0)
    {
      //Cache values for iterate1
      cl_enqueue_kernel(P->cl, P->kernel_cache, &P->profile1.event);   //Profile this kernel
    }

    else
    {
      //Split the job up if requested
      int BLOCK = (dspec.nFG / P->divide + 0.5);
    
      //printf("iterate1 0 ");
      for (int start=0; start<dspec.nFG; start += BLOCK)
      {
        int end = start + BLOCK;
        if (end > dspec.nFG) end = dspec.nFG;
        //printf("%d ", end);
        cl_set_arg(cl, P->kernel_iterate1, 15, start);
        cl_set_arg(cl, P->kernel_iterate1, 16, end);
        cl_enqueue_kernel(P->cl, P->kernel_iterate1, &P->profile1.event);   //Profile this kernel
      }
      //printf("\n");
    }
    cl_set_arg(cl, P->kernel_delta_b, 2, P->dspec.start);
    cl_set_arg(cl, P->kernel_delta_b, 3, P->dspec.end);
    cl_enqueue_kernel(P->cl, P->kernel_delta_b, NULL);
    cl_run(P->cl);
    //Profile
    cl_profile(P->cl, &P->profile1);
#elif defined(USE_OPENMP)
    // if (omp_get_dynamic()) {
    //   printroot ("dynamic threads enabled in OPENMP\n");
    // }

    memset(P->Ax_b, 0, (P->dN) * sizeof(Real));
    memset(P->Dx, 0, (P->dN) * sizeof(Real));
    if (rank==0) printroot("Num of CPU: %d\n", omp_get_num_procs());
    if (rank==0) printroot("Max threads: %d\n", omp_get_max_threads());
    // TODO(timseries): can't scope these class data members dspec, kernel, P, and rank, which is bad omp pracitce..., find a better solution!

#pragma omp parallel shared(nthreads,chunk) private(tid,o,p,ox,oy,oz,px,py,pz,rx,ry,rz,_rx,_ry,_rz,mix)
    {
      tid = omp_get_thread_num();
      if (tid == 0 && rank==0 && iteration==0)
      {
        nthreads = omp_get_num_threads();
        printf("Starting  A*x-b using %d threads\n",nthreads);
        printf("Initializing matrices...\n");
      }
#pragma omp for
    for (o = 0; o < dspec.nFG; o++) {
      if (P->x[o]) {
        
        oz = FGindices[o] / dspec.zoffset;
        oy = (FGindices[o] - oz * dspec.zoffset) / dspec.yoffset;
        ox = FGindices[o] - oy * dspec.yoffset - oz * dspec.zoffset;

        // TODO(timseries): find out why these were missing from OK's implementation.
        pz = dStart / dspec.zoffset;
        py = (dStart - pz * dspec.zoffset) / dspec.yoffset;
        px = dStart - py * dspec.yoffset - pz * dspec.zoffset - 1;			    

        ry = py - oy;
        rz = pz - oz;

        _rx = abs(rx);
        _ry = abs(ry);
        _rz = abs(rz);
        
        for (p = dStart; p < dEnd; p++) {
          px++;
          if (px == dspec.size[0]) {
            px = 0;
            py++;
            if (py == dspec.size[1]) {
              py = 0;
              pz++;
              rz = pz - oz;
              _rz = abs(rz);
            }
            ry = py - oy;
            _ry = abs(ry);
          }        
          rx = px - ox;
          _rx = abs(rx);
          
          if (_rx <= kernel.halfsize && _ry <= kernel.halfsize && _rz <= kernel.halfsize) {
            // Linear system
            mix = kernel.modelmap.mask[FGindices[o]];
            if (mix == -1) { // spherical kernel
              P->Ax_b[p - dStart] += kernel.skernel[rx+kernel.halfsize + (ry+kernel.halfsize)*kernel.yoffset + (rz+kernel.halfsize)*kernel.zoffset] * P->x[o];
            }
            else if (P->PreCalcCylinders) {
              P->Ax_b[p - dStart] += cylColumns[mix*dN + p-dStart] * P->x[o];
            }
            else if (rx == 0 && ry == 0 && rz == 0) {
              P->Ax_b[p - dStart] += kernel.ctr[mix] * P->x[o];
            }
            else {
              P->Ax_b[p - dStart] += kernel.GetCyl(mix, rx, ry, rz) * P->x[o];
            }
            
            //OK 7/13: Optimised Laplacian,
            //This is not faster on CPU (as it is on GPU due to reduced branching)
            //but it is cleaner, using pre-calculated single precision constants also slightly faster
            if (_rx <= 1 && _ry <= 1 && _rz <= 1)
            {
              P->Dx[p-dStart] += Lfactors[_rx + _ry + _rz] * P->x[o];
            }
          }
        }
      }
    }
#pragma omp for
    for (p = 0; p < P->dN; p++) {
      P->Ax_b[p] -= deltab[p];
    }
}//end omp parallel section
#else
    memset(P->Ax_b, 0, (P->dN) * sizeof(Real));
    memset(P->Dx, 0, (P->dN) * sizeof(Real));

    for (o = 0; o < dspec.nFG; o++) {
      if (P->x[o]) {
        
        oz = FGindices[o] / dspec.zoffset;
        oy = (FGindices[o] - oz * dspec.zoffset) / dspec.yoffset;
        ox = FGindices[o] - oy * dspec.yoffset - oz * dspec.zoffset;

        // TODO(timseries): find out why these were missing from OK's implementation.
        pz = dStart / dspec.zoffset;
        py = (dStart - pz * dspec.zoffset) / dspec.yoffset;
        px = dStart - py * dspec.yoffset - pz * dspec.zoffset - 1;			    

        ry = py - oy;
        rz = pz - oz;

        _rx = abs(rx);
        _ry = abs(ry);
        _rz = abs(rz);
        
        for (p = dStart; p < dEnd; p++) {
          px++;
          if (px == dspec.size[0]) {
            px = 0;
            py++;
            if (py == dspec.size[1]) {
              py = 0;
              pz++;
              rz = pz - oz;
              _rz = abs(rz);
            }
            ry = py - oy;
            _ry = abs(ry);
          }        
          rx = px - ox;
          _rx = abs(rx);
          
          if (_rx <= kernel.halfsize && _ry <= kernel.halfsize && _rz <= kernel.halfsize) {
            // Linear system
            mix = kernel.modelmap.mask[FGindices[o]];
            if (mix == -1) { // spherical kernel
              P->Ax_b[p - dStart] += kernel.skernel[rx+kernel.halfsize + (ry+kernel.halfsize)*kernel.yoffset + (rz+kernel.halfsize)*kernel.zoffset] * P->x[o];
            }
            else if (P->PreCalcCylinders) {
              P->Ax_b[p - dStart] += cylColumns[mix*dN + p-dStart] * P->x[o];
            }
            else if (rx == 0 && ry == 0 && rz == 0) {
              P->Ax_b[p - dStart] += kernel.ctr[mix] * P->x[o];
            }
            else {
              P->Ax_b[p - dStart] += kernel.GetCyl(mix, rx, ry, rz) * P->x[o];
            }
            
            //OK 7/13: Optimised Laplacian,
            //This is not faster on CPU (as it is on GPU due to reduced branching)
            //but it is cleaner, using pre-calculated single precision constants also slightly faster
            if (_rx <= 1 && _ry <= 1 && _rz <= 1)
            {
              P->Dx[p-dStart] += Lfactors[_rx + _ry + _rz] * P->x[o];
            }
          }
        }
      }
    }
    for (p = 0; p < P->dN; p++) {
      P->Ax_b[p] -= deltab[p];
    }
#endif
    //end old fullpass process, Ax_b = A * x - b, Dx = D * x

    // Begin old fullpass process, AtAx_b = A' * Ax_b, DtDx = D' * Dx
#ifdef USE_OPENCL
    P->profile2.kern_time = 0;
    cl_size(P->cl, P->dspec.nFG, 0, P->threads, rank);  //Resize
    //Split the job up if requested
    int BLOCK = (dN / P->divide + 0.5);
    //printf("iterate2 0 ");
    for (int start=dStart; start<P->dEnd; start += BLOCK)
    {
      int end = start + BLOCK;
      if (end > dEnd) end = dEnd;
      //printf("%d ", end);
      cl_set_arg(cl, P->kernel_iterate2, 15, start);
      cl_set_arg(cl, P->kernel_iterate2, 16, end);
      cl_set_arg(cl, P->kernel_iterate2, 17, dStart);
      // Perform the operations
      cl_enqueue_kernel(P->cl, P->kernel_iterate2, &P->profile2.event);   //Profile this kernel
    }
    //printf("\n");
    // Read the results back
    cl_enqueue_read(P->cl, P->cl_AtAx_b, P->cl_size_fg, P->AtAx_b);
    cl_enqueue_read(P->cl, P->cl_DtDx, P->cl_size_fg, P->DtDx);
    //Run queued operations
    cl_run(P->cl);
    //Profile
    cl_profile(P->cl, &P->profile2);
#elif defined(USE_OPENMP)
    // if (omp_get_dynamic()) {
    //   printroot ("dynamic threads enabled in OPENMP\n");
    // }

    memset(P->AtAx_b, 0, P->dspec.nFG*sizeof(Real));
    memset(P->DtDx, 0, P->dspec.nFG*sizeof(Real));
    ///if (rank==0) printroot("Num of CPU: %d\n", omp_get_num_procs());
    //if (rank==0) printroot("Max threads: %d\n", omp_get_max_threads());
    // TODO(timseries): can't scope these class data members dspec, kernel, P, and rank, which is bad omp pracitce..., find a better solution!
    //    int chunk = 10;
    //    int tid, nthreads;
    //    int o,p,ox,oy,oz,px,py,pz,rx,ry,rz;

#pragma
 omp parallel shared(nthreads,chunk) private(tid,o,p,ox,oy,oz,px,py,pz,rx,ry,rz,_rx,_ry,_rz,mix)
  {
      tid = omp_get_thread_num();
      if (tid == 0 && rank==0 && iteration==0)
      {
        nthreads = omp_get_num_threads();
        printf("Starting  AtAx_b using %d threads\n",nthreads);
        printf("Initializing matrices...\n");
      }
#pragma omp for
    for (o = 0; o < dspec.nFG; o++) {
      oz = FGindices[o] / dspec.zoffset;
      oy = (FGindices[o] - oz * dspec.zoffset) / dspec.yoffset;
      ox = FGindices[o] - oy * dspec.yoffset - oz * dspec.zoffset;
      
      pz = dStart / dspec.zoffset;
      py = (dStart - pz * dspec.zoffset) / dspec.yoffset;
      px = dStart - py * dspec.yoffset - pz * dspec.zoffset - 1;      
      
      ry = py - oy;
      rz = pz - oz;

      _rx = abs(rx);
      _ry = abs(ry);
      _rz = abs(rz);

      
      //roffset = (ry+kernel.halfsize)*kernel.yoffset + (rz+kernel.halfsize)*kernel.zoffset;
      
      for (p = dStart; p < dEnd; p++) {
        
        px++;
        if (px == dspec.size[0]) {
          px = 0;
          py++;
          if (py == dspec.size[1]) {
            py = 0;
            pz++;
            rz = pz - oz;
            _rz = abs(rz);
          }
          ry = py - oy;
          _ry = abs(ry);
        }        
        rx = px - ox;
          _rx = abs(rx);
     
        if (_rx <= kernel.halfsize && _ry <= kernel.halfsize && _rz <= kernel.halfsize) {
     
          // Linear system
          mix = kernel.modelmap.mask[FGindices[o]];

          if (mix == -1) { // spherical kernel
            P->AtAx_b[o] += kernel.skernel[rx+kernel.halfsize + (ry+kernel.halfsize)*kernel.yoffset + (rz+kernel.halfsize)*kernel.zoffset] * P->Ax_b[p - dStart];
          }
          else if (P->PreCalcCylinders) {
            P->AtAx_b[o] += cylColumns[mix*dN + p-dStart] * P->Ax_b[p - dStart];
          }
          else if (rx == 0 && ry == 0 && rz == 0) {
            P->AtAx_b[o] += kernel.ctr[mix] * P->Ax_b[p - dStart];
          }
          else {
            P->AtAx_b[o] += kernel.GetCyl(mix, rx, ry, rz) * P->Ax_b[p - dStart];
          }
  
          //OK 7/13: Optimised Laplacian,
          //This is not faster on CPU (as it is on GPU due to reduced branching)
          //but it is cleaner, using pre-calculated single precision constants also slightly faster
          if (_rx <= 1 && _ry <= 1 && _rz <= 1)
          {
            P->DtDx[o] += Lfactors[_rx + _ry + _rz] * P->Dx[p-dStart];
          }
        }
      }
    }
  } //end omp parallel section
#else
    memset(P->AtAx_b, 0, P->dspec.nFG*sizeof(Real));
    memset(P->DtDx, 0, P->dspec.nFG*sizeof(Real));
    //LWIterate(P->AtAx_b, P->Ax_b, 1, P->DtDx, P->Dx);
    for (o = 0; o < dspec.nFG; o++) {
      oz = FGindices[o] / dspec.zoffset;
      oy = (FGindices[o] - oz * dspec.zoffset) / dspec.yoffset;
      ox = FGindices[o] - oy * dspec.yoffset - oz * dspec.zoffset;
      
      pz = dStart / dspec.zoffset;
      py = (dStart - pz * dspec.zoffset) / dspec.yoffset;
      px = dStart - py * dspec.yoffset - pz * dspec.zoffset - 1;      
      
      ry = py - oy;
      rz = pz - oz;

      int _rx = abs(rx);
      int _ry = abs(ry);
      int _rz = abs(rz);

      
      //roffset = (ry+kernel.halfsize)*kernel.yoffset + (rz+kernel.halfsize)*kernel.zoffset;
      
      for (p = dStart; p < dEnd; p++) {
        
        px++;
        if (px == dspec.size[0]) {
          px = 0;
          py++;
          if (py == dspec.size[1]) {
            py = 0;
            pz++;
            rz = pz - oz;
            _rz = abs(rz);
          }
          ry = py - oy;
          _ry = abs(ry);
        }        
        rx = px - ox;
          _rx = abs(rx);
     
        if (_rx <= kernel.halfsize && _ry <= kernel.halfsize && _rz <= kernel.halfsize) {
     
          // Linear system
          int mix = kernel.modelmap.mask[FGindices[o]];

          if (mix == -1) { // spherical kernel
            P->AtAx_b[o] += kernel.skernel[rx+kernel.halfsize + (ry+kernel.halfsize)*kernel.yoffset + (rz+kernel.halfsize)*kernel.zoffset] * P->Ax_b[p - dStart];
          }
          else if (P->PreCalcCylinders) {
            P->AtAx_b[o] += cylColumns[mix*dN + p-dStart] * P->Ax_b[p - dStart];
          }
          else if (rx == 0 && ry == 0 && rz == 0) {
            P->AtAx_b[o] += kernel.ctr[mix] * P->Ax_b[p - dStart];
          }
          else {
            P->AtAx_b[o] += kernel.GetCyl(mix, rx, ry, rz) * P->Ax_b[p - dStart];
          }
  
          //OK 7/13: Optimised Laplacian,
          //This is not faster on CPU (as it is on GPU due to reduced branching)
          //but it is cleaner, using pre-calculated single precision constants also slightly faster
          if (_rx <= 1 && _ry <= 1 && _rz <= 1)
          {
            P->DtDx[o] += Lfactors[_rx + _ry + _rz] * P->Dx[p-dStart];
          }
        }
      }
    }
#endif
    // End old fullpass process, AtAx_b = A' * Ax_b, DtDx = D' * Dx
